#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Author: zty
#  本代码实现爬取小说排行榜的所有小说并保存下来
import tools
import re
def getBookUrl(webUrl):  # webUrl是指排行榜页面的地址  返回书本的地址列表
    webSoup = tools.getSoup(webUrl)
    # print(webSoup)
    bookUrls = webSoup.find('div',class_='main_m')
    # print(bookUrls)
    op = []
    books = []
    for ea in bookUrls:
       url = re.findall(r'href="(/book/.*?)"',str(ea))
       op.append(url)  #op[1]里面是所有的地址  每一个地址重复2遍  类型为list
    # print(op[3])
    for num in range(0,len(op[3])):
        if num%2 == 0:
            books.append('https://www.bookbao99.net'+op[3][num])
    # print(books)
    return books
def getChapUrl(bookUrl):
    soup = tools.getSoup(bookUrl)
    chapurls = soup.find('div',class_='wp b2 info_chapterlist')
    chapurlx = re.findall(r'href="(.*?)"',str(chapurls))
    chapurlist = []
    for ea in chapurlx:
        chapurlist.append('https://www.bookbao99.net'+ea)
    # print(chapurlist)
    return chapurlist
def getBookTitle(bookurl):
    soup = tools.getSoup(bookurl)
    title = soup.find('li',class_='am-active').getText()
    return title
def getChap(chapurl):
    soup = tools.getSoup(chapurl)
    fin = soup.find('div', class_='bdsub').getText()
    x = re.findall(r'&gt;(.*?)&lt', fin, re.S)
    pp = str(x).replace(r'\n\n', '\n').replace(r'\xa0', '')
    # print(pp)
    return pp
    # print(op)
    # print(bookUrls)
if __name__ == '__main__':
    webUrl = 'https://www.bookbao99.net/BookList-c_2-t_0-o_0.html'
    bookurlist = getBookUrl(webUrl)
    for a in bookurlist:         #a是一本书的链接
        chapurlist = getChapUrl(a)
        titile = getBookTitle(a)
        file = open(r'C:/Users/Administrator/Desktop/'+titile+'.txt','w')
        i = 1
        for b in chapurlist:
            chap = getChap(b)
            print('第'+str(i)+'章')
            file.write('第'+str(i)+'章')
            file.write('\n')
            file.write(str(chap).replace("['",'').replace("']",''))
            file.write('\n')
            i = i+1
        print(titile+'下载完成')
        file.close()



